# LLaVa-PLLaMa: CropCare Vision Language Model for Potato Disease Detection

We started with the LLaVa framework, which uses a vision encoder ([CLIP-Vit](https://openai.com/index/clip/)) and a language model ([Vicuna](https://huggingface.co/lmsys/vicuna-13b-v1.5-16k)) as the text decoder. The Vicuna model is a fine-tuned version of LLaMa-2, but we have modified the architecture by replacing Vicuna with [PLLaMa](https://arxiv.org/abs/2401.01600) to better suit our needs. The resulting LLaVa-PLLaMa model is trained to align visual features from the CLIP-ViT model with textual representations generated by PLLaMa.

## Pretraining and Fine-Tuning

### Pretraining

Our pretraining strategy involves the following steps:

1. **Image Encoder Pretraining:** The model is pretrained on the CLIP-ViT image encoder. This step allows the model to learn how to align visual features from the CLIP-ViT model with PLLaMaâ€™s textual features.
   - CLIP-ViT is a powerful image encoder that learns to map images into a shared embedding space with text.
   - This pretraining enables the model to effectively process both images and text in a unified representation.

2. **Vision and Text Alignment:** The pretraining phase ensures that the model can align the visual features from the CLIP-ViT encoder with the textual representations generated by PLLaMa, fostering robust multimodal understanding.

### Fine-Tuning

After pretraining, we fine-tune the model using our custom Image-Text dataset. This dataset contains images classified into categories such as 'Healthy', 'Virus', 'Fungal', and more. Each image is accompanied by descriptive text.

## Model Configuration and Implementation

### LLaVa-Vicuna Codebase

We cloned the [LLaVa-Vicuna 1.6 repository](https://github.com/haotian-liu/LLaVA) to customize the model architecture, specifically replacing the Vicuna text decoder with PLLaMa. The LLaVa framework allows for easy modification of both the text decoder and image encoder, making it ideal for our project.

### CLIP-ViT Image Encoder

The image encoder used in this project is based on the CLIP-ViT model. CLIP learns visual features from natural language descriptions, making it well-suited for aligning image and text embeddings. The [CLIP ViT-L/14 model](https://huggingface.co/openai/clip-vit-large-patch14) is used to process images and extract relevant visual features.
